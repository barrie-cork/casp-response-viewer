# CASP Response Viewer - Instructor Guide

A comprehensive guide for using the CASP Response Viewer to facilitate effective classroom discussions about critical appraisal.

## Table of Contents

1. [Pedagogical Approach](#pedagogical-approach)
2. [Pre-Class Preparation](#pre-class-preparation)
3. [During Class: Facilitation Strategies](#during-class-facilitation-strategies)
4. [Post-Class: Data Analysis](#post-class-data-analysis)
5. [Discussion Prompts](#discussion-prompts)
6. [Common Student Mistakes](#common-student-mistakes)
7. [Time Management](#time-management)

## Pedagogical Approach

### Why This Tool Works

**Peer Learning**: Students learn more from comparing multiple reasoning approaches than from hearing a single "correct" answer. Seeing how peers interpreted the same paper helps students recognize gaps in their own critical thinking.

**Metacognition**: Anonymized responses encourage honest self-reflection. Students can think "I didn't consider that factor" without feeling judged.

**Active Engagement**: Voting requires students to evaluate quality of reasoning, not just passively receive information.

**Formative Assessment**: You can quickly identify which concepts the entire class struggles with, allowing real-time curriculum adjustments.

### Learning Objectives

By the end of a response viewer session, students should be able to:
1. Identify characteristics of exemplary critical appraisal explanations
2. Recognize common reasoning patterns (both strong and weak)
3. Articulate what makes one explanation more comprehensive than another
4. Apply these insights to improve their own future appraisals

## Pre-Class Preparation

### 1 Week Before Class

- [ ] Assign CASP form as homework
- [ ] Specify which RCT paper students should appraise
- [ ] Set submission deadline (24 hours before class)
- [ ] Share CASP resources and tutorials if needed

### 24 Hours Before Class

- [ ] Check that at least 80% of students have submitted
- [ ] Open Google Sheet and verify responses are complete
- [ ] Test viewer URL on projector in classroom
- [ ] Review responses to identify interesting discussion points
- [ ] Note questions where students had most "Can't Tell" responses
- [ ] Identify exemplary explanations to highlight

### 30 Minutes Before Class

- [ ] Open viewer URL on classroom projector
- [ ] Test that responses load correctly
- [ ] Verify voting system works
- [ ] Share URL with students (QR code or short link)
- [ ] Have backup plan (PDF of responses) in case of technical issues

## During Class: Facilitation Strategies

### Session Structure (90-minute class)

**Intro (5 minutes)**
- Explain purpose: Learn from peers' reasoning approaches
- Emphasize anonymity encourages honest learning
- Demonstrate voting: "Vote for explanations that are comprehensive, specific, and well-reasoned"

**Question-by-Question Discussion (70 minutes)**
- ~5-7 minutes per question (prioritize most important questions)
- Focus on Questions 1, 2, 3, 5, 7, 10 if time limited

**Synthesis (10 minutes)**
- Review voting data
- Highlight patterns in exemplary responses
- Identify most common gaps

**Reflection (5 minutes)**
- Students note what they'll improve in future appraisals
- Preview next assignment

### Facilitation Techniques

#### Starting Each Question

1. **Show the Question**
   - Read question aloud
   - Click "Show CONSIDER Prompts" to display evaluation criteria
   - Ask: "What are we looking for in a good answer here?"

2. **Give Reading Time**
   - Display all responses (don't filter yet)
   - Allow 2-3 minutes for silent reading
   - Encourage students to open URL on their devices

3. **Initial Reactions**
   - Ask: "What patterns do you notice?"
   - "Which explanations stood out to you?"
   - "Are there common elements in the stronger responses?"

#### Comparing Responses

4. **Use Filters Strategically**
   - Filter to show only "Can't Tell" responses
   - Discuss: "When is 'Can't Tell' appropriate vs. avoiding analysis?"
   - Compare "Yes" vs "No" responses to same paper
   - Ask: "Why might students disagree on this question?"

5. **Highlight Exemplary Reasoning**
   - Point to specific strong explanations
   - Ask: "What makes this explanation comprehensive?"
   - Look for: Specific citations, consideration of alternatives, clear logic

6. **Identify Common Gaps**
   - Point out what many responses missed
   - Ask: "What did the best responses include that others didn't?"
   - Common gaps: Not citing specific paper sections, making assumptions, vague language

#### Encouraging Participation

7. **Think-Pair-Share**
   - "Take 30 seconds to identify the strongest response"
   - "Turn to neighbor and explain your choice"
   - "Now vote for your top choice"

8. **Ask for Justifications**
   - After voting: "Who voted for Student 5's response? Why?"
   - "What specific phrases or reasoning convinced you?"
   - "How could this explanation be even stronger?"

9. **Challenge Assumptions**
   - "Student 12 said 'Can't Tell' - is that justified?"
   - "Student 7 said randomization was adequate - what evidence supports this?"
   - "How could we verify this interpretation?"

### Question-Specific Strategies

#### Question 1 (Research Question)

**What to look for**: Mention of PICO (Population, Intervention, Comparison, Outcome)

**Discussion prompts**:
- "How many responses explicitly mentioned PICO?"
- "Can we infer the research question was clear even if not perfectly stated?"
- "What's the difference between 'formulated' and just 'stated'?"

**Common mistakes**:
- Saying "yes" without evidence
- Not distinguishing between clinical question and research question
- Assuming PICO if not explicitly stated

#### Question 2 (Randomization)

**What to look for**: Specific randomization method, allocation concealment, baseline comparability

**Discussion prompts**:
- "Did anyone find where the paper described the randomization method?"
- "Is 'computer-generated randomization' enough detail?"
- "What's the difference between randomization and blinding?"

**Common mistakes**:
- Confusing randomization with random selection
- Not checking for allocation concealment
- Assuming adequate randomization if paper just says "randomized"

#### Question 3 (Participant Accounting)

**What to look for**: CONSORT diagram, intention-to-treat analysis, attrition rates

**Discussion prompts**:
- "How many responses mentioned the CONSORT diagram?"
- "What attrition rate is acceptable?"
- "Does this study use intention-to-treat or per-protocol analysis?"

**Common mistakes**:
- Not checking for differential attrition between groups
- Missing secondary exclusions after randomization
- Not considering impact of missing data

#### Questions 4a-4c (Blinding)

**What to look for**: Understanding of single/double/triple blind, justification for blinding feasibility

**Discussion prompts**:
- "When is blinding NOT feasible? Does that make the study invalid?"
- "How does lack of blinding affect different outcomes?"
- "What's the difference between blinding participants vs outcome assessors?"

**Common mistakes**:
- Thinking all RCTs must be blinded
- Not considering whether blinding was possible
- Confusing blinding with allocation concealment

#### Question 5 (Baseline Similarity)

**What to look for**: Reference to baseline tables, consideration of prognostic factors

**Discussion prompts**:
- "Which baseline characteristics are most important for this intervention?"
- "Small imbalances - do they matter?"
- "How do we know if randomization worked?"

**Common mistakes**:
- Only checking age and sex
- Ignoring disease severity or comorbidities
- Not considering whether differences could affect outcomes

#### Question 7 (Comprehensive Reporting)

**What to look for**: Power calculation, effect sizes, confidence intervals, secondary outcomes, harms

**Discussion prompts**:
- "What outcomes were NOT reported?"
- "Were harms reported as thoroughly as benefits?"
- "Did the study report what they said they would in the methods?"

**Common mistakes**:
- Only looking at primary outcome
- Not checking for selective reporting
- Missing information about harms

#### Question 10 (Local Applicability)

**What to look for**: Consideration of population differences, resource constraints, contextual factors

**Discussion prompts**:
- "How is YOUR population different from the study population?"
- "What barriers to implementation exist in your setting?"
- "Are the outcomes measured relevant to your patients?"

**Common mistakes**:
- Assuming all evidence applies everywhere
- Not considering healthcare system differences
- Ignoring resource constraints

### Managing Difficult Situations

#### Low Participation

**If students aren't voting**:
- Make voting mandatory: "Everyone vote before we move on"
- Lower stakes: "There's no wrong answer - we're learning together"
- Model thinking: "I'd vote for Student 8 because they cited specific page numbers"

#### Defensiveness

**If a student recognizes their response and feels embarrassed**:
- Emphasize learning: "These are drafts - we all improve through feedback"
- Normalize mistakes: "I see this pattern every year - it's a common learning step"
- Celebrate growth: "Now you know what to improve next time"

#### Time Running Short

**If falling behind schedule**:
- Skip Questions 4b, 4c, 6, 8 if needed
- Focus on Questions 1, 2, 3, 5, 7, 10
- Use filters to quickly show patterns instead of discussing each response

#### Technical Issues

**If viewer not loading**:
- Have backup PDF of responses
- Use Google Sheets directly (less ideal but functional)
- Reschedule discussion if completely non-functional

## Post-Class: Data Analysis

### Reviewing Voting Data

1. **Open Google Sheet â†’ "Votes" tab**
2. **Create pivot table**:
   - Rows: QuestionIndex
   - Values: Count of Timestamp
   - Identify which explanations got most votes

3. **Analyze patterns**:
   - Which students consistently got high votes?
   - What characteristics do highly-voted responses share?
   - Which questions had least voting activity (may indicate confusion)?

### Identifying Learning Gaps

**High "Can't Tell" responses** (>50% of class):
- Paper may have poor reporting quality
- OR students may not know where to look for information
- Follow-up: Show where information WAS in paper, or acknowledge genuine reporting gap

**Split "Yes"/"No" responses**:
- May indicate ambiguous paper writing
- OR students interpreting criteria differently
- Follow-up: Discuss how to handle borderline cases

**Empty explanations**:
- Students may be rushing
- OR may not understand what's expected
- Follow-up: Provide exemplar explanations for next assignment

### Preparing for Next Session

- Share anonymized exemplar responses with class
- Create FAQ based on common misconceptions
- Adjust rubric to emphasize what students missed
- Consider providing paper with better reporting quality next time

## Discussion Prompts

### General Prompts

- "What evidence from the paper supports this interpretation?"
- "What would make this explanation more convincing?"
- "How could we verify this claim?"
- "What alternative interpretations exist?"
- "What did this response include that others didn't?"

### Critical Thinking Prompts

- "Is 'Can't Tell' justified here, or should we infer from context?"
- "Does absence of information mean it wasn't done, or just not reported?"
- "How certain can we be based on the evidence provided?"
- "What additional information would you need?"

### Comparative Prompts

- "Compare Student 3 and Student 15 - which is more thorough?"
- "These both say 'Yes' but give different reasons - which is stronger?"
- "Why might someone answer 'No' to this question?"

## Common Student Mistakes

### 1. Making Assumptions

**Mistake**: "The study was probably randomized properly even though they didn't describe the method"

**Teach**: If information isn't explicit, answer "Can't Tell". Don't give benefit of doubt.

### 2. Confusing Related Concepts

**Mistake**: Mixing up randomization, blinding, allocation concealment

**Teach**: Use precise terminology. Draw diagrams showing differences.

### 3. Vague Language

**Mistake**: "The study was good" or "They explained it well"

**Teach**: Cite specific page numbers, quote exact phrases, name specific elements (CONSORT, PICO, etc.)

### 4. Binary Thinking

**Mistake**: Assuming every flaw makes study invalid

**Teach**: Consider magnitude of problems. Perfect studies don't exist.

### 5. Not Consulting Primary Source

**Mistake**: Relying on abstract instead of full paper

**Teach**: Most critical information is in methods and supplementary materials

## Time Management

### 60-Minute Class (Condensed)

- Intro: 3 min
- Cover 6 priority questions: 8 min each (48 min)
- Synthesis: 6 min
- Reflection: 3 min

**Priority questions**: 1, 2, 3, 5, 7, 10

### 90-Minute Class (Standard)

- Intro: 5 min
- Cover 10-11 questions: 6-7 min each (70 min)
- Synthesis: 10 min
- Reflection: 5 min

**Skip if needed**: Questions 4b, 4c, 6, 8

### 120-Minute Class (Comprehensive)

- Intro: 10 min
- Cover all 13 questions: 7-8 min each (95 min)
- Synthesis: 10 min
- Reflection: 5 min

## Success Metrics

Your session is successful if students:
- [ ] Can articulate what makes an explanation comprehensive
- [ ] Recognize at least 3 characteristics of exemplary responses
- [ ] Identify at least 1 thing they'll improve in future appraisals
- [ ] Engage with peers' reasoning (not just their own)
- [ ] Ask clarifying questions about CASP criteria

## Further Resources

- **CASP Workshops**: https://casp-uk.net/casp-workshops/
- **CONSORT Statement**: http://www.consort-statement.org/
- **Cochrane Handbook**: https://training.cochrane.org/handbook
- **Critical Appraisal Tools**: https://www.cebm.ox.ac.uk/resources/ebm-tools

---

**Remember**: The goal isn't to criticize student responses, but to build a shared understanding of what quality critical appraisal looks like. Use the viewer to facilitate discovery, not to lecture.

Happy facilitating! ðŸŽ“
